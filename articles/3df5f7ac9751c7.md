---
title: "pysparkÂÖ•ÈñÄ"
emoji: "üéá"
type: "tech"
topics:
  - "python"
  - "pyspark"
  - "spark"
  - "pandas"
published: true
published_at: "2023-04-04 01:37"
---

‚ÄªJupyter Notebook ÂΩ¢Âºè„ÅßÂÆüË°åÔºà[ÂéüÊú¨](https://gist.github.com/junkor-1011/d54da7ec98a56c0905952daba82a2b4b#file-pyspark-test-ipynb)Ôºâ„Åó„ÄÅMarkdown ÂΩ¢Âºè„Åß„Ç®„ÇØ„Çπ„Éù„Éº„Éà„Åó„Åü„ÇÇ„ÅÆ„Çí‰∏ÄÈÉ®ÊîπÂ§â„Åó„Å¶‰Ωú„Å£„Å¶„ÅÑ„Çã„ÅÆ„ÅßËã•Âπ≤Ë¶ã„Å•„Çâ„ÅÑ„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„Çì„Åå„ÅîÂÆπËµ¶„Åè„Å†„Åï„ÅÑ

## Ê¶ÇË¶Å

pandas „Å™„Å©„ÅØÂ∞èË¶èÊ®°~‰∏≠Ë¶èÊ®°„Åè„Çâ„ÅÑ„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂá¶ÁêÜ„ÇÑÂàÜÊûê„Å´„ÅØÊúâÂäπ„Å†„Åå„ÄÅ
„É°„É¢„É™„Å´‰πó„ÇäÂàá„Çâ„Å™„ÅÑ„É¨„Éô„É´„ÅÆ„Éá„Éº„ÇøÈáè„ÅØÂá¶ÁêÜ„Åß„Åç„Å™„ÅÑ„ÄÇ
„Åæ„Åü„ÄÅGIL „Å´„Çà„Å£„Å¶Âü∫Êú¨ÁöÑ„Å´ 1 „Çπ„É¨„ÉÉ„Éâ„Åß„ÅÆÂãï‰Ωú„Å´„Å™„Çã„Åü„ÇÅ„ÄÅ„Éû„Ç∑„É≥„Çπ„Éö„ÉÉ„ÇØ„ÇíÊúÄÂ§ßÈôêÊ¥ª„Åã„ÅóÂàá„Çã„ÅÆ„ÅØÈõ£„Åó„ÅÑ„ÄÇ

ÈÅÖÂª∂Ë©ï‰æ°„ÄÅ‰∏¶ÂàóÂàÜÊï£Âá¶ÁêÜ„Å´„Çà„Å£„Å¶ÊâÄË¨Ç„Éì„ÉÉ„Ç∞„Éá„Éº„Çø„Å®„ÅÑ„Çè„Çå„Çã„É¨„Éô„É´„ÅÆ„ÉÜ„Éº„Éñ„É´„Éá„Éº„Çø„ÅÆÂá¶ÁêÜ„ÉªÂàÜÊûê„Å´‰Ωø„ÅÜ„Åì„Å®„Åå„Åß„Åç„ÄÅ
Êõ¥„Å´ pandas „Å®„ÅÆÈÄ£Êê∫„Éª‰ΩµÁî®„Åå„Åß„Åç„Çã„ÉÑ„Éº„É´„Å®„Åó„Å¶[pyspark](https://www.databricks.com/jp/glossary/pyspark)„ÅåÂ≠òÂú®„Åô„Çã„Åü„ÇÅÁ¥π‰ªã„Åô„Çã„ÄÇ

„Å™„Åä„ÄÅË©≥Á¥∞„ÅØ[PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)„Å™„Å©„ÇÇÂèÇÁÖß„ÅÆ„Åì„Å®

## Áí∞Â¢É„ÅÆ‰ΩúÊàê

Mambaforge „Çí„Ç§„É≥„Çπ„Éà„Éº„É´Ê∏à„Åø„ÅÆ Linux Áí∞Â¢É„ÇíÂâçÊèê„Å®„Åô„Çã„ÄÇ

- conda 23.1.0
- mamba 1.4.1

‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™ conda ‰ªÆÊÉ≥Áí∞Â¢É‰ΩúÊàêÁî®„ÅÆ yaml „ÇíÁî®ÊÑè„Åô„Çã:

```yaml:pyspark_env.yaml
# pyspark_env.yaml
name: pyspark-intro
channels:
  - conda-forge
dependencies:
  - python=3.11
  - pyspark=3.3
  - openjdk=17
  - pandas=1.5
  - pyarrow
  # for testdata
  - numpy
  - seaborn
  # jupyter
  - jupyterlab
```

[ÔºàÂèÇËÄÉÔºâ](https://qiita.com/junkor-1011/items/7ec9bfaaf76568ce4a05)

‚ÄªÂÖàÊó•[pandas „ÅÆ 2.0 „Åå„É™„É™„Éº„Çπ„Åï„Çå„Åü](https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html)„Åå„ÄÅ
deprecated „Å´„Å™„Å£„Å¶„ÅÑ„Åü[`iteritems`](https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html#removal-of-prior-version-deprecations-changes)„ÅåÂâäÈô§„Åï„Çå„ÅüÂΩ±Èüø„Åß„ÄÅ
pandas „Åã„Çâ pyspark „ÅÆ DataFrame „Å´Â§âÊèõ„Åô„ÇãÈÉ®ÂàÜ„Åå‰∏ÄÈÉ®Âãï‰Ωú„Åó„Å™„Åè„Å™„ÇãÁèæË±°„ÅåËµ∑„Åì„Çã„Åü„ÇÅ„ÄÅÂΩìÈù¢„ÅØ 1.5 Á≥ª„Çí‰Ωø„ÅÜ„ÄÇ

mamba „Ç≥„Éû„É≥„Éâ„Åß‰ªÆÊÉ≥Áí∞Â¢É„Çí‰ΩúÊàê(conda „Å†„Å®‰æùÂ≠òÈñ¢‰øÇ„ÅÆËß£Ê±∫„ÅåÈÅÖ„ÅÑ„ÅÆ„Åß mamba „ÅÆ‰ΩøÁî®„ÇíÊé®Â•®)

```sh
mamba env create -f pyspark_env.yaml

# Âá∫Êù•‰∏ä„Åå„Å£„ÅüÁí∞Â¢É(pyspark-intro)„Çíactivate
conda activate pyspark-intro
# or `mamba activate pyspark-intro`
```

## ÂÆüË°å

- spark session „ÅÆËµ∑Âãï
- „Éá„Éº„Çø„ÅÆË™≠„ÅøËæº„Åø„Å®‰øùÂ≠ò
- pandas „Å®„ÅÆÈÄ£Êê∫

„ÅÇ„Åü„Çä„ÇíÁ∞°Âçò„Å´Á¢∫Ë™ç„Åô„Çã„ÄÇ

„Éá„Éº„ÇøÂá¶ÁêÜÂë®„Çä„ÅÆË©≥Á¥∞„ÅØÁúÅÁï•„Åô„Çã„Åå„ÄÅSpark „ÅÆ„Éá„Éº„Çø„Éï„É¨„Éº„É†Ê©üËÉΩ„Åå[Spark SQL](https://www.databricks.com/jp/glossary/what-is-spark-sql)„Å®ÂØæÂøú„Åó„Å¶„ÅÑ„Çã„Åì„Å®„Åã„Çâ„ÄÅ
Ê¶Ç„Å≠ SQL „ÅßÂá∫Êù•„Çã„Åì„Å®„Çà„ÅÜ„Å™„Åì„Å®„ÇíÂÆüË°åÂá∫Êù•„Çã„Å®ËÄÉ„Åà„Çå„Å∞ OK„ÄÇÔºà„Å®„ÅÑ„ÅÜ„Åã SQL „Åß„ÇÇÊõ∏„Åë„ÇãÔºâ

‰ª•‰∏ã„ÄÅ‰∏äË®ò„ÅÆÊâãÈ†Ü„Åß‰Ωú„Å£„Åü conda ‰ªÆÊÉ≥Áí∞Â¢É„Åã„Çâ Jupyter „ÇíËµ∑Âãï„Åó„Å¶ÂÆüË°å„Åô„Çã„ÄÇ

```python
from pyspark.sql.session import SparkSession

# spark session„ÅÆÁ´ã„Å°‰∏ä„Åí
spark = (SparkSession.builder
  .master("local[*]")
  .appName("LocalTest")
  .getOrCreate()
)
spark
```

![](https://storage.googleapis.com/zenn-user-upload/58066df2aa11-20230404.png)

‚Üìpandas „Å®„ÅÆÈÄ£Êê∫„ÅÆ„Åü„ÇÅ„ÄÅconfig „ÇíË™øÊï¥„Åó„Å¶„Åä„Åè(pyarrow „Çí„Ç§„É≥„Çπ„Éà„Éº„É´„Åó„Å¶„Åä„Åè)

ÂèÇËÄÉ: https://learn.microsoft.com/ja-jp/azure/databricks/pandas/pyspark-pandas-conversion

```python
spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")
```

Spark Session „ÇíËµ∑Âãï„Åô„Çã„Å®„ÄÅ„Éá„Éï„Ç©„É´„Éà„Åß„ÅØ[http://localhost:4040](http://localhost:4040)„Å´[Spark UI](http://localhost:4040)„ÅåÁ´ã„Å°‰∏ä„Åå„Çä„ÄÅ
ÂÆüË°åÁä∂ÊÖã„Å™„Å©„Çí„É¢„Éã„Çø„É™„É≥„Ç∞„Åô„Çã„Åì„Å®„ÅåÂá∫Êù•„ÇãÔºà„Éá„Éê„ÉÉ„Ç∞„Å™„Å©„Å´„ÇÇ‰æøÂà©Ôºâ

```python
# „ÉÜ„Çπ„Éà„Éá„Éº„Çø„ÅÆ‰ΩúÊàê
import seaborn as sns

iris = sns.load_dataset("iris")
display(iris)

iris.to_csv("testdata/iris.csv", index=False)
```

![](https://storage.googleapis.com/zenn-user-upload/99c390c6f8e3-20230404.png)

```python
# pyspark„Åßcsv„ÇíË™≠„ÅøËæº„Åø

df_iris = spark.read.csv(
    "testdata/iris.csv",
    header=True,
    inferSchema=True,
)

# ÈÅÖÂª∂Ë©ï‰æ°„Åô„Çã„ÅÆ„Åß„ÄÅ„Ç´„É©„É†„ÅÆ„Çπ„Ç≠„Éº„Éû„ÅÆ„ÅøË°®Á§∫
display(df_iris)
```

    DataFrame[sepal_length: string, sepal_width: string, petal_length: string, petal_width: string, species: string]

‚Äª(Ë£úË∂≥):

- ÈÅÖÂª∂Ë©ï‰æ°„ÇíË°å„ÅÜÈñ¢‰øÇ„Åß„ÄÅ„Ç™„É≥„É°„É¢„É™„Å´Â±ïÈñã„Åô„Çã pandas „Å™„Å©„Å®Áï∞„Å™„Çä Spark „ÅÆÂá¶ÁêÜ„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÅØ„Éá„Éº„Çø„ÇΩ„Éº„Çπ„ÅÆÂΩ¢Âºè„ÅÆÂΩ±Èüø„Çí„ÇÇ„Çç„Å´Âèó„Åë„Çã
  - „Éá„Éº„ÇøÂΩ¢Âºè(csv/json/parquet/orc/RDB „ÉÜ„Éº„Éñ„É´/...etc)„ÇÑ„ÄÅÂúßÁ∏ÆÂΩ¢Âºè(gzip/xz/snappy/zlib/...etc)„ÄÅ„Ç´„É©„É†„Å´ÂØæ„Åô„Çã„Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„Å™„Å©
- „Éá„Éº„ÇøÂàÜÊûê„Éª„Éá„Éº„ÇøÂá¶ÁêÜÁî®ÈÄî„Åß„ÅÇ„Çå„Å∞Ë°åÊåáÂêëÂΩ¢Âºè„Åß„ÅÇ„Çã csv „ÇíË™≠„ÅøËæº„Çì„ÅßÂá¶ÁêÜ„ÇíÂÆüË°å„Åô„Çã„ÅÆ„ÅØ‰∏ÄËà¨„Å´ÈÅÖ„Åè„ÄÅÂàóÊåáÂêëÂΩ¢Âºè„ÅÆ parquet „ÇÑ orc „Å™„Å©„ÅÆÊñπ„ÅåÊúâÂà©

```python
# show„Å™„Å©„Å´„Çà„Å£„Å¶Âàù„ÇÅ„Å¶Ë©ï‰æ°„ÅåË°å„Çè„Çå„Çã
df_iris.show(5) # 5‰ª∂„ÇíË°®Á§∫
```

    +------------+-----------+------------+-----------+-------+
    |sepal_length|sepal_width|petal_length|petal_width|species|
    +------------+-----------+------------+-----------+-------+
    |         5.1|        3.5|         1.4|        0.2| setosa|
    |         4.9|        3.0|         1.4|        0.2| setosa|
    |         4.7|        3.2|         1.3|        0.2| setosa|
    |         4.6|        3.1|         1.5|        0.2| setosa|
    |         5.0|        3.6|         1.4|        0.2| setosa|
    +------------+-----------+------------+-----------+-------+
    only showing top 5 rows

```python
# select„Å´„Çà„Çã„Ç´„É©„É†„ÅÆÈÅ∏Êäû
(df_iris
    .select(["sepal_length", "sepal_width"])
    .show(3)
)
```

    +------------+-----------+
    |sepal_length|sepal_width|
    +------------+-----------+
    |         5.1|        3.5|
    |         4.9|        3.0|
    |         4.7|        3.2|
    +------------+-----------+
    only showing top 3 rows

```python
# filter„Å´„Çà„ÇãÁ∞°Âçò„Å™„ÇØ„Ç®„É™
(df_iris
    .filter(df_iris.species == "virginica")
    .filter(df_iris.sepal_length > 5.0)
    .show(3)
)
```

    +------------+-----------+------------+-----------+---------+
    |sepal_length|sepal_width|petal_length|petal_width|  species|
    +------------+-----------+------------+-----------+---------+
    |         6.3|        3.3|         6.0|        2.5|virginica|
    |         5.8|        2.7|         5.1|        1.9|virginica|
    |         7.1|        3.0|         5.9|        2.1|virginica|
    +------------+-----------+------------+-----------+---------+
    only showing top 3 rows

```python
# groupBy„Å´„Çà„ÇãÈõÜÁ¥Ñ(1)
(df_iris
    .groupBy('species')
    .count()
    .show()
)
```

    +----------+-----+
    |   species|count|
    +----------+-----+
    | virginica|   50|
    |versicolor|   50|
    |    setosa|   50|
    +----------+-----+

```python
# groupBy„Å´„Çà„ÇãÈõÜÁ¥Ñ(2)

(df_iris
    .groupBy('species')
    .agg({
        'sepal_length': 'mean',
        'sepal_width': 'mean',
        'petal_length': 'mean',
        'petal_width': 'mean',
    })
    .show()
)
```

    +----------+------------------+------------------+-----------------+------------------+
    |   species|  avg(sepal_width)|  avg(petal_width)|avg(sepal_length)| avg(petal_length)|
    +----------+------------------+------------------+-----------------+------------------+
    | virginica|2.9739999999999998|             2.026|6.587999999999998|             5.552|
    |versicolor|2.7700000000000005|1.3259999999999998|            5.936|              4.26|
    |    setosa| 3.428000000000001|0.2459999999999999|5.005999999999999|1.4620000000000002|
    +----------+------------------+------------------+-----------------+------------------+

```python
# DataFrame„ÅÆ„Çµ„Éû„É™„Éº
df_iris.describe().show()
```

    +-------+------------------+-------------------+------------------+------------------+---------+
    |summary|      sepal_length|        sepal_width|      petal_length|       petal_width|  species|
    +-------+------------------+-------------------+------------------+------------------+---------+
    |  count|               150|                150|               150|               150|      150|
    |   mean| 5.843333333333335|  3.057333333333334|3.7580000000000027| 1.199333333333334|     null|
    | stddev|0.8280661279778637|0.43586628493669793|1.7652982332594662|0.7622376689603467|     null|
    |    min|               4.3|                2.0|               1.0|               0.1|   setosa|
    |    max|               7.9|                4.4|               6.9|               2.5|virginica|
    +-------+------------------+-------------------+------------------+------------------+---------+

```python
# SQL„Åß„ÇØ„Ç®„É™„ÇíÊõ∏„Åè„Åì„Å®„ÇÇÂèØËÉΩ
df_iris.createOrReplaceTempView('iris')

spark.sql("show tables").show()

tmp = spark.sql("SELECT * FROM iris WHERE species = 'setosa'")
tmp.show()
```

    +---------+---------+-----------+
    |namespace|tableName|isTemporary|
    +---------+---------+-----------+
    |         |     iris|       true|
    +---------+---------+-----------+

    +------------+-----------+------------+-----------+-------+
    |sepal_length|sepal_width|petal_length|petal_width|species|
    +------------+-----------+------------+-----------+-------+
    |         5.1|        3.5|         1.4|        0.2| setosa|
    |         4.9|        3.0|         1.4|        0.2| setosa|
    |         4.7|        3.2|         1.3|        0.2| setosa|
    |         4.6|        3.1|         1.5|        0.2| setosa|
    |         5.0|        3.6|         1.4|        0.2| setosa|
    |         5.4|        3.9|         1.7|        0.4| setosa|
    |         4.6|        3.4|         1.4|        0.3| setosa|
    |         5.0|        3.4|         1.5|        0.2| setosa|
    |         4.4|        2.9|         1.4|        0.2| setosa|
    |         4.9|        3.1|         1.5|        0.1| setosa|
    |         5.4|        3.7|         1.5|        0.2| setosa|
    |         4.8|        3.4|         1.6|        0.2| setosa|
    |         4.8|        3.0|         1.4|        0.1| setosa|
    |         4.3|        3.0|         1.1|        0.1| setosa|
    |         5.8|        4.0|         1.2|        0.2| setosa|
    |         5.7|        4.4|         1.5|        0.4| setosa|
    |         5.4|        3.9|         1.3|        0.4| setosa|
    |         5.1|        3.5|         1.4|        0.3| setosa|
    |         5.7|        3.8|         1.7|        0.3| setosa|
    |         5.1|        3.8|         1.5|        0.3| setosa|
    +------------+-----------+------------+-----------+-------+
    only showing top 20 rows

Ë©≥Á¥∞„ÅØ[User Guide](https://spark.apache.org/docs/3.3.2/api/python/user_guide/index.html), [API Reference](https://spark.apache.org/docs/3.3.2/api/python/reference/index.html),
„Åä„Çà„Å≥[Spark SQL „ÅÆ„É™„Éï„Ç°„É¨„É≥„Çπ](https://spark.apache.org/docs/3.3.2/api/sql/index.html)„ÇíÂèÇÁÖß

Ê¨°„Å´„ÄÅ„Éá„Éº„Çø„ÅÆ[Read/Write](https://spark.apache.org/docs/latest/sql-data-sources.html)„ÇíÁ¢∫Ë™ç„Åô„Çã

```python
# „Åï„Å£„Åç„ÅØ‰æøÂÆú‰∏äcsv„Åã„ÇâË™≠„ÅøËæº„Çì„Å†„Åå„ÄÅÂûãÊÉÖÂ†±„ÅåÂ§±„Çè„Çå„Çã + pandas„Å®„ÅÆÈÄ£Êê∫„ÇíË¶ã„Çã„Åü„ÇÅ„ÄÅpandas„ÅÆ„Éá„Éº„Çø„Éï„É¨„Éº„É†„Åã„ÇâÁõ¥Êé•pyspark„Å´Â§âÊèõ„Åô„Çã

display(iris.__class__) # iris„ÅØpandas dataframe
df = spark.createDataFrame(iris)

display(df)

df.show()
```

    /home/wsl-user/LocalApps/Mambaforge/envs/pyspark-intro/lib/python3.11/site-packages/pyspark/pandas/__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.
      warnings.warn(



    pandas.core.frame.DataFrame


    /home/wsl-user/LocalApps/Mambaforge/envs/pyspark-intro/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.
      [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]



    DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string]


    +------------+-----------+------------+-----------+-------+
    |sepal_length|sepal_width|petal_length|petal_width|species|
    +------------+-----------+------------+-----------+-------+
    |         5.1|        3.5|         1.4|        0.2| setosa|
    |         4.9|        3.0|         1.4|        0.2| setosa|
    |         4.7|        3.2|         1.3|        0.2| setosa|
    |         4.6|        3.1|         1.5|        0.2| setosa|
    |         5.0|        3.6|         1.4|        0.2| setosa|
    |         5.4|        3.9|         1.7|        0.4| setosa|
    |         4.6|        3.4|         1.4|        0.3| setosa|
    |         5.0|        3.4|         1.5|        0.2| setosa|
    |         4.4|        2.9|         1.4|        0.2| setosa|
    |         4.9|        3.1|         1.5|        0.1| setosa|
    |         5.4|        3.7|         1.5|        0.2| setosa|
    |         4.8|        3.4|         1.6|        0.2| setosa|
    |         4.8|        3.0|         1.4|        0.1| setosa|
    |         4.3|        3.0|         1.1|        0.1| setosa|
    |         5.8|        4.0|         1.2|        0.2| setosa|
    |         5.7|        4.4|         1.5|        0.4| setosa|
    |         5.4|        3.9|         1.3|        0.4| setosa|
    |         5.1|        3.5|         1.4|        0.3| setosa|
    |         5.7|        3.8|         1.7|        0.3| setosa|
    |         5.1|        3.8|         1.5|        0.3| setosa|
    +------------+-----------+------------+-----------+-------+
    only showing top 20 rows

‚Üëcsv „Åã„ÇâË™≠„ÅøËæº„Çì„Å†„Å®„Åç„ÅØ string Âûã„Å´„Å™„Å£„Å¶„Åó„Åæ„Å£„Å¶„ÅÑ„ÅüÊï∞ÂÄ§„Åå„ÄÅpandas „ÅÆÂûã„ÇíÂèçÊò†„Åó„Å¶ double Âûã„Å´„Å™„Å£„Å¶„ÅÑ„Çã

```python
# spark dataframe„Åã„Çâpandas dataframe„Å∏„ÅÆÂ§âÊèõ

display(df.__class__) # df„ÅØpyspark dataframe

pdf = df.toPandas()
display(pdf)
```

    pyspark.sql.dataframe.DataFrame

![](https://storage.googleapis.com/zenn-user-upload/e3a1d8216c07-20230404.png)

‚Üëpyspark „ÅßÂ§ßË¶èÊ®°~‰∏≠Ë¶èÊ®°„Åè„Çâ„ÅÑ„ÅÆ„Éá„Éº„Çø„ÇíÂá¶ÁêÜ„Åó„Å¶„Çµ„Ç§„Ç∫„ÉÄ„Ç¶„É≥„Åó„ÄÅ„Ç™„É≥„É°„É¢„É™„Å´‰πó„Çã„Åè„Çâ„ÅÑ„Å´„Å™„Å£„Åü„Çâ`toPandas`„Çí„Åó„Å¶Êâ±„ÅÑ„ÇÑ„Åô„ÅÑ pandas „Å´Â§âÊèõ„Åô„Çã„ÄÅ„Å®„ÅÑ„ÅÜ‰Ωø„ÅÑÊñπ„ÅåÂá∫Êù•„Çã„ÄÇ

ÊúÄÂæå„Å´Ëâ≤„ÄÖ„Å™ÂΩ¢Âºè„Åß„Éá„Éº„Çø„ÅÆÊõ∏„ÅçËæº„Åø„Å®Ë™≠„ÅøËæº„Åø„ÇíË°å„ÅÜ

ÂÖàËø∞„ÅÆÈÄö„Çä„ÄÅspark „ÅØ„Éá„Éº„Çø‰øùÂ≠òÂΩ¢Âºè„ÅåÂá¶ÁêÜ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Å´„ÉÄ„Ç§„É¨„ÇØ„Éà„Å´Âäπ„ÅÑ„Å¶„Åè„Çã„Åü„ÇÅ„ÄÅ„Åì„ÅÆËæ∫„Çä„ÅÆÂèñ„ÇäÊâ±„ÅÑ„ÅØËÇù„Å´„Å™„Å£„Å¶„Åè„Çã„ÄÇ

```python
# df„ÅØspark dataframe

# csv.gz
df.write.save("testdata/iris_csv", format="csv")

# format„Ç™„Éó„Ç∑„Éß„É≥„Çí‰Ωø„ÅÜ‰ª£„Çè„Çä„Å´„ÄÅ‚Üì„ÅÆ„Çà„ÅÜ„Å´Êõ∏„ÅÑ„Å¶„ÇÇOK
# df.write.csv("testdata/iris_csv", compression="gzip")

# ÂÆüÈöõ„Å©„ÅÆ„Çà„ÅÜ„Å™ÂΩ¢Âºè„Åß„Éï„Ç°„Ç§„É´„Åå‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„ÅãÁ¢∫Ë™ç(linux„Ç≥„Éû„É≥„Éâ„ÇíÂÆüË°å„Åó„Å¶„ÅÑ„Çã)
!ls testdata/iris_csv
```

    _SUCCESS
    part-00000-030e186f-051c-4e9d-94b1-1560776ebfef-c000.csv
    part-00001-030e186f-051c-4e9d-94b1-1560776ebfef-c000.csv
    part-00002-030e186f-051c-4e9d-94b1-1560776ebfef-c000.csv
    part-00003-030e186f-051c-4e9d-94b1-1560776ebfef-c000.csv
    part-00004-030e186f-051c-4e9d-94b1-1560776ebfef-c000.csv
    part-00005-030e186f-051c-4e9d-94b1-1560776ebfef-c000.csv
    part-00006-030e186f-051c-4e9d-94b1-1560776ebfef-c000.csv
    part-00007-030e186f-051c-4e9d-94b1-1560776ebfef-c000.csv

```python
# ‰∏ÄÂøú‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Çã„Éï„Ç°„Ç§„É´„ÅÆ1„Å§„Å´„Å§„ÅÑ„Å¶ÂÖàÈ†≠„ÇíË¶ã„Å¶„Åø„Çã
!head testdata/iris_csv/part-00000-*.csv
```

    5.1,3.5,1.4,0.2,setosa
    4.9,3.0,1.4,0.2,setosa
    4.7,3.2,1.3,0.2,setosa
    4.6,3.1,1.5,0.2,setosa
    5.0,3.6,1.4,0.2,setosa
    5.4,3.9,1.7,0.4,setosa
    4.6,3.4,1.4,0.3,setosa
    5.0,3.4,1.5,0.2,setosa
    4.4,2.9,1.4,0.2,setosa
    4.9,3.1,1.5,0.1,setosa

‚Üëpandas „ÅÆ„Çà„ÅÜ„Å´Âçò‰∏Ä„ÅÆ„Éï„Ç°„Ç§„É´„ÇíÊåáÂÆö„Åô„ÇãÊÑü„Åò„Åß„ÅØ„Å™„Åè„ÄÅ‰øùÂ≠ò„Åô„Çã„Éá„Ç£„É¨„ÇØ„Éà„É™„ÇíÊåáÂÆö„Åô„Çã„ÄÇ
(„Éï„Ç°„Ç§„É´„ÅåÂàÜÊï£„Åó„Å¶ÁîüÊàê„Åï„Çå„Çã)

Ë™≠„ÅøËæº„ÇÄÈöõ„ÅØÂÄã„ÄÖ„ÅÆ„Éï„Ç°„Ç§„É´„ÇíÊåáÂÆö„Åô„Çã„Åì„Å®„ÇÇ„Åß„Åç„Çã(‚ÜêSpark ‰ª•Â§ñ„Åß‰Ωú„Å£„Åü„Éï„Ç°„Ç§„É´„ÅÆË™≠„ÅøËæº„Åø„Å™„Å©)„Åå„ÄÅ
**Âü∫Êú¨„ÅØ‰øùÂ≠ò„Åó„Åü„Éá„Ç£„É¨„ÇØ„Éà„É™„ÇíÊåáÂÆö„Åó„Å¶Ë™≠„ÅøËæº„ÇÄ**

```python
df_csv = spark.read.load("testdata/iris_csv", format="csv")

# format„ÇíÊåáÂÆö„Åõ„Åö„ÄÅ‚Üì„ÅÆ„Çà„ÅÜ„Å´„Åô„Çã„Åì„Å®„ÇÇÂèØËÉΩ
# df_csv = spark.read.csv("testdata/iris_csv")

df_csv.show(3)
```

    +---+---+---+---+----------+
    |_c0|_c1|_c2|_c3|       _c4|
    +---+---+---+---+----------+
    |4.9|2.4|3.3|1.0|versicolor|
    |6.6|2.9|4.6|1.3|versicolor|
    |5.2|2.7|3.9|1.4|versicolor|
    +---+---+---+---+----------+
    only showing top 3 rows

(‚Üëcsv „Å†„Å®„Ç´„É©„É†Âêç„Å™„Å©„ÅÆÊÉÖÂ†±„ÅåÊ¨†ËêΩ„Åó„Åå„Å°„Åß‰Ωø„ÅÑ„Å•„Çâ„ÅÑ)

```python
# „ÉÜ„Ç≠„Çπ„ÉàÂΩ¢Âºè„Å†„Å®csv„ÅÆ‰ªñ„Å´json„Å™„Å©„ÇÇÊåáÂÆöÂèØËÉΩ
# compression„ÅßÂúßÁ∏ÆÂΩ¢Âºè„ÇíÊåáÂÆö„Åô„Çã„Åì„Å®„ÇÇÂá∫Êù•„Çã

df.write.json("testdata/iris_json", compression='gzip')

!ls testdata/iris_json
```

    _SUCCESS
    part-00000-005412a7-1f0e-4ebf-a4d9-bfbbe53b0515-c000.json.gz
    part-00001-005412a7-1f0e-4ebf-a4d9-bfbbe53b0515-c000.json.gz
    part-00002-005412a7-1f0e-4ebf-a4d9-bfbbe53b0515-c000.json.gz
    part-00003-005412a7-1f0e-4ebf-a4d9-bfbbe53b0515-c000.json.gz
    part-00004-005412a7-1f0e-4ebf-a4d9-bfbbe53b0515-c000.json.gz
    part-00005-005412a7-1f0e-4ebf-a4d9-bfbbe53b0515-c000.json.gz
    part-00006-005412a7-1f0e-4ebf-a4d9-bfbbe53b0515-c000.json.gz
    part-00007-005412a7-1f0e-4ebf-a4d9-bfbbe53b0515-c000.json.gz

```python
# Ë™≠„ÅøËæº„Åø„ÇÇÂêåÊßò

df_json = spark.read.json("testdata/iris_json")
# or df_json = spark.read.load("testdata/iris_json", format="json")

display(df_json)
df_json.show(3)
```

    DataFrame[petal_length: double, petal_width: double, sepal_length: double, sepal_width: double, species: string]


    +------------+-----------+------------+-----------+----------+
    |petal_length|petal_width|sepal_length|sepal_width|   species|
    +------------+-----------+------------+-----------+----------+
    |         4.2|        1.2|         5.7|        3.0|versicolor|
    |         4.2|        1.3|         5.7|        2.9|versicolor|
    |         4.3|        1.3|         6.2|        2.9|versicolor|
    +------------+-----------+------------+-----------+----------+
    only showing top 3 rows

(‚Üëjson „Å†„Å® csv „Çà„Çä„ÅØÂûãÊÉÖÂ†±„Åå‰øùÊåÅ„Åï„Çå„Çã)

```python
# „Éá„Éº„ÇøÂàÜÊûê„ÉªÂá¶ÁêÜÁî®ÈÄî„Å®„Åó„Å¶ÈÅ©„Åó„Å¶„ÅÑ„Çãsnappy.parquet„ÇÑzlib.orcÂΩ¢Âºè„Å™„Å©

df.write.parquet("testdata/iris_parquet", compression="snappy")
!ls testdata/iris_parquet

df.write.orc("testdata/iris_orc", compression="zlib")
!ls testdata/iris_orc
```

    23/04/04 01:07:23 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory
    Scaling row group sizes to 95.00% for 8 writers




    _SUCCESS
    part-00000-ccc33d97-5a54-492a-9cd2-12ae8adb52ea-c000.snappy.parquet
    part-00001-ccc33d97-5a54-492a-9cd2-12ae8adb52ea-c000.snappy.parquet
    part-00002-ccc33d97-5a54-492a-9cd2-12ae8adb52ea-c000.snappy.parquet
    part-00003-ccc33d97-5a54-492a-9cd2-12ae8adb52ea-c000.snappy.parquet
    part-00004-ccc33d97-5a54-492a-9cd2-12ae8adb52ea-c000.snappy.parquet
    part-00005-ccc33d97-5a54-492a-9cd2-12ae8adb52ea-c000.snappy.parquet
    part-00006-ccc33d97-5a54-492a-9cd2-12ae8adb52ea-c000.snappy.parquet
    part-00007-ccc33d97-5a54-492a-9cd2-12ae8adb52ea-c000.snappy.parquet
    _SUCCESS
    part-00000-0fbad17c-8b1f-42ce-a3b2-06d173e191ed-c000.zlib.orc
    part-00001-0fbad17c-8b1f-42ce-a3b2-06d173e191ed-c000.zlib.orc
    part-00002-0fbad17c-8b1f-42ce-a3b2-06d173e191ed-c000.zlib.orc
    part-00003-0fbad17c-8b1f-42ce-a3b2-06d173e191ed-c000.zlib.orc
    part-00004-0fbad17c-8b1f-42ce-a3b2-06d173e191ed-c000.zlib.orc
    part-00005-0fbad17c-8b1f-42ce-a3b2-06d173e191ed-c000.zlib.orc
    part-00006-0fbad17c-8b1f-42ce-a3b2-06d173e191ed-c000.zlib.orc
    part-00007-0fbad17c-8b1f-42ce-a3b2-06d173e191ed-c000.zlib.orc

```python
# Ë™≠„ÅøËæº„Åø

# parquet
print("parquet:")
df_parquet = spark.read.parquet("testdata/iris_parquet")
display(df_parquet)
df_parquet.show(3)

print("\n------------------\n")

# orc
print("orc:")
df_orc = spark.read.orc("testdata/iris_orc")
display(df_orc)
df_orc.show(3)
```

    parquet:



    DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string]


    +------------+-----------+------------+-----------+----------+
    |sepal_length|sepal_width|petal_length|petal_width|   species|
    +------------+-----------+------------+-----------+----------+
    |         5.7|        3.0|         4.2|        1.2|versicolor|
    |         5.7|        2.9|         4.2|        1.3|versicolor|
    |         6.2|        2.9|         4.3|        1.3|versicolor|
    +------------+-----------+------------+-----------+----------+
    only showing top 3 rows


    ------------------

    orc:



    DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string]


    +------------+-----------+------------+-----------+----------+
    |sepal_length|sepal_width|petal_length|petal_width|   species|
    +------------+-----------+------------+-----------+----------+
    |         5.7|        3.0|         4.2|        1.2|versicolor|
    |         5.7|        2.9|         4.2|        1.3|versicolor|
    |         6.2|        2.9|         4.3|        1.3|versicolor|
    +------------+-----------+------------+-----------+----------+
    only showing top 3 rows

```python
# partition„ÇíÂàá„Å£„Å¶‰øùÂ≠ò„Åô„Çã„Åì„Å®„ÇÇÂèØËÉΩ

df.write.save(
    "testdata/iris_with_partition",
    format="parquet",
    compression="snappy",
    partitionBy="species"
    )

!cd testdata/iris_with_partition && tree
```

.
‚îú‚îÄ‚îÄ \_SUCCESS
‚îú‚îÄ‚îÄ species=setosa
‚îÇ ‚îú‚îÄ‚îÄ part-00000-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet
‚îÇ ‚îú‚îÄ‚îÄ part-00001-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet
‚îÇ ‚îî‚îÄ‚îÄ part-00002-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet
‚îú‚îÄ‚îÄ species=versicolor
‚îÇ ‚îú‚îÄ‚îÄ part-00002-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet
‚îÇ ‚îú‚îÄ‚îÄ part-00003-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet
‚îÇ ‚îú‚îÄ‚îÄ part-00004-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet
‚îÇ ‚îî‚îÄ‚îÄ part-00005-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet
‚îî‚îÄ‚îÄ species=virginica
‚îú‚îÄ‚îÄ part-00005-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet
‚îú‚îÄ‚îÄ part-00006-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet
‚îî‚îÄ‚îÄ part-00007-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet

    4 directories, 11 files

„Ç´„É©„É†"species"„ÅÆÂÄ§„Åî„Å®„Å´Âà•„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅåÂàá„Çâ„Çå„Å¶„Éá„Éº„Çø„Åå‰øùÂ≠ò„Åï„Çå„Çã

RDB „ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Å®ÂêåÊßò„ÄÅ„Éá„Éº„Çø„Ç¢„ÇØ„Çª„Çπ„ÅÆ‰ªïÊñπ„Å´Âøú„Åò„Å¶ÈÅ©Âàá„Å´Ë®≠ÂÆö„Åô„Çã„Åì„Å®„Åß„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÇíËâØ„Åè„Åô„Çã„Åì„Å®„ÅåÂá∫Êù•„ÇãÔºà„Åó„ÄÅ‰∏çÈÅ©Âàá„Å†„Å®ÊÇ™Âåñ„Åô„ÇãÔºâ

‚Üë „ÅÆ‰æã„Å†„Å®„ÄÅspecies „ÇíÊåáÂÆö„Åó„Å¶ÂàÜÊûê„ÇíË°å„ÅÜ„Çà„ÅÜ„Å™Â†¥Âêà„ÄÅËààÂë≥„ÅÆ„ÅÇ„Çã species „Åó„ÅãË¶ã„Å™„ÅÑ„Åü„ÇÅ„Ç¢„ÇØ„Çª„Çπ„Åô„Çã„Éá„Éº„ÇøÈáè„ÇíÈôêÂÆö„Åô„Çã„Åì„Å®„Åå„Åß„Åç„Çã„ÄÇ
‰∏ÄÊñπ„ÄÅ„Åã„Åë„Çã„ÇØ„Ç®„É™„ÅÆÂ§ö„Åè„ÅåË§áÊï∞„ÅÆ species „Å´„Åæ„Åü„Åå„Çã„Çà„ÅÜ„Å™Â†¥Âêà„ÅØ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÅÆÂêë‰∏ä„ÅØË¶ãËæº„ÇÅ„Åö„ÄÅÊÇ™Âåñ„Åô„ÇãÂèØËÉΩÊÄß„ÇÇ„ÅÇ„Çã„ÄÇ

```python
# „Éë„Éº„ÉÜ„Ç£„Ç∑„Éß„É≥„Åå„ÅÇ„ÇãÂ†¥Âêà„Åß„ÇÇË™≠„ÅøËæº„ÅøÊñπÊ≥ï„ÅØÂêåÊßò

df_partition = spark.read.load("testdata/iris_with_partition")

display(df_partition)
df_partition.show(3)
```

    DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string]


    +------------+-----------+------------+-----------+---------+
    |sepal_length|sepal_width|petal_length|petal_width|  species|
    +------------+-----------+------------+-----------+---------+
    |         5.8|        2.8|         5.1|        2.4|virginica|
    |         6.4|        3.2|         5.3|        2.3|virginica|
    |         6.5|        3.0|         5.5|        1.8|virginica|
    +------------+-----------+------------+-----------+---------+
    only showing top 3 rows

‰ªñ„Å´„ÇÇ[saveAsTable](https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.DataFrameWriter.saveAsTable.html)„ÅßÊ∞∏Á∂öÂåñ„Åó„Åü„ÉÜ„Éº„Éñ„É´„Å®„Åó„Å¶Êõ∏„ÅçËæº„Åø„ÇíË°å„Å£„Åü„Çä„ÇÇÂá∫Êù•„Çã„ÄÇ

Êâ±„Åà„Çã„Éá„Éº„Çø‰øùÂ≠òÂΩ¢Âºè„ÇÇ[Data Sources](https://spark.apache.org/docs/latest/sql-data-sources.html)„ÅÆ„Çà„ÅÜ„Å´„ÄÅ
‰ªñ„Å´„ÇÇ RDB(JDBC „Çí‰Ωø„ÅÜ)„ÇÑ„ÄÅHive Table „Å™„Å©Ëâ≤„ÄÖ„Å™„ÇÇ„ÅÆ„Å´ÂØæÂøú„Åó„Å¶„ÅÑ„Çã„ÄÇ

## (Ë£úË∂≥)pyspark „Å´„Åä„Åë„Çã pandas API

ÂèÇËÄÉ: https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/index.html

ÊúÄËøë„ÅØ„Åã„Å™„Çä pandas „Å´Ëøë„ÅÑ API „Çí‰Ωø„Å£„Å¶ spark „ÇíÊâ±„ÅÜÊñπÊ≥ï„ÅåÊï¥ÂÇô„Åï„Çå„Å§„Å§„ÅÇ„ÇãÊßòÂ≠ê

„Å™„Åä„ÄÅ[Koalas](https://github.com/databricks/koalas)„Å®„ÅÑ„ÅÜ„ÄÅspark „Çí pandas „Å£„ÅΩ„Åè‰Ωø„Åà„Çã„ÉÑ„Éº„É´„ÅåÂ≠òÂú®„Åô„Çã„Åå„ÄÅ
[„Åì„Åì](https://learn.microsoft.com/ja-jp/azure/databricks/archive/legacy/koalas)„Å™„Å©„ÇíË¶ã„Çã„Å®„Åì„Çå„ÅåÊ≠£Âºè„Å´ pyspark „Å´Âèñ„ÇäËæº„Åæ„Çå„ÅüÔºüÊßòÂ≠ê„ÄÇ

```python
import pyspark.pandas as ps

# pandas on spark DataFrame„Çí‰ΩúÊàê„Åô„Çã
psdf = ps.read_csv("testdata/iris.csv")

display(psdf.__class__) # pyspark.pandas.frame.DataFrame

psdf.head(5) # pandas DataFrame„ÅÆ„Çà„ÅÜ„Å´head„ÅßÂÖàÈ†≠„ÇíË°®Á§∫„Åß„Åç„Çã
```

    /home/wsl-user/LocalApps/Mambaforge/envs/pyspark-intro/lib/python3.11/site-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.
      warnings.warn(message, PandasAPIOnSparkAdviceWarning)



    pyspark.pandas.frame.DataFrame

![](https://storage.googleapis.com/zenn-user-upload/d915a20ea87b-20230404.png)

```python
# filter„ÇÑË°®Á§∫„ÇÇÈÄöÂ∏∏„ÅÆpandas„ÅÆ„Çà„ÅÜ„Å´„Åß„Åç„Å¶„ÅÑ„Çã
psdf[psdf['sepal_length'] > 7.5]
```

![](https://storage.googleapis.com/zenn-user-upload/ba8a910f46ba-20230404.png)

```python
# pyspark„ÅÆDataFrame„Å´Â§âÊèõ

sdf = psdf.to_spark(index_col=["index"])

sdf.show(5)
```

    +-----+------------+-----------+------------+-----------+-------+
    |index|sepal_length|sepal_width|petal_length|petal_width|species|
    +-----+------------+-----------+------------+-----------+-------+
    |    0|         5.1|        3.5|         1.4|        0.2| setosa|
    |    1|         4.9|        3.0|         1.4|        0.2| setosa|
    |    2|         4.7|        3.2|         1.3|        0.2| setosa|
    |    3|         4.6|        3.1|         1.5|        0.2| setosa|
    |    4|         5.0|        3.6|         1.4|        0.2| setosa|
    +-----+------------+-----------+------------+-----------+-------+
    only showing top 5 rows

```python
# ÊôÆÈÄö„ÅÆpandas„Å´Â§âÊèõ
pdf = psdf.to_pandas()

display(pdf.__class__) # pandas.core.frame.DataFrame

pdf.head()
```

    /home/wsl-user/LocalApps/Mambaforge/envs/pyspark-intro/lib/python3.11/site-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.
      warnings.warn(message, PandasAPIOnSparkAdviceWarning)



    pandas.core.frame.DataFrame

![](https://storage.googleapis.com/zenn-user-upload/db76402dc9ee-20230404.png)

## (ÂèÇËÄÉ) [dask](https://www.dask.org/)„Å®„ÅÆÊØîËºÉ

pandas „Åä„Çà„Å≥ numpy „Å®‰∫íÊèõÊÄß„Åä„Çà„Å≥ÈÄ£Êê∫ÊÄß„ÅÆÈ´ò„ÅÑ API „ÇíÊåÅ„Å°„ÄÅÈÅÖÂª∂Ë©ï‰æ°„Éª‰∏¶ÂàóÂàÜÊï£Âá¶ÁêÜ„ÇíË°å„ÅÜ„Åì„Å®„ÅåÂá∫Êù•„Çã„É©„Ç§„Éñ„É©„É™„ÄÇ

‰æã„Åà„Å∞„Éá„Éº„Çø„Çµ„Ç§„Ç∫„ÅåÂ§ß„Åç„ÅÑ„ÉÜ„Éº„Éñ„É´„Éá„Éº„Çø„Å´ÂØæ„Åó„Å¶„ÅØ dask „ÅÆ„Éá„Éº„Çø„Éï„É¨„Éº„É†„ÅßÈÅÖÂª∂Ë©ï‰æ°„Éª‰∏¶ÂàóÂàÜÊï£Âá¶ÁêÜ„ÇíË°å„Å£„Å¶„Åä„Åç„ÄÅ
„Éá„Éº„Çø„Çµ„Ç§„Ç∫„ÅåÂ∞è„Åï„Åè„Å™„Çã„Çø„Ç§„Éü„É≥„Ç∞„Åß pandas „ÅÆ„Éá„Éº„Çø„Éï„É¨„Éº„É†„Å´Â§âÊèõ„Åô„Çã„ÄÅ„Å®„ÅÑ„Å£„Åü‰ªäÂõû pyspark „Åß„ÇÑ„Çç„ÅÜ„Å®„Åó„Å¶„ÅÑ„Çã„Åì„Å®„Å®„Åª„ÅºÂêå„Åò„Åì„Å®„ÅåÂá∫Êù•„Çã„ÄÇ

pyspark „ÅØ dask „Å´ÊØî„Åπ„Çã„Å®„ÄÅ

- „ÇØ„É©„Çπ„Çø„Éº„Å™„Å©„ÇíÁµÑ„ÇÅ„ÇãÂàÜ„ÄÅ„Çà„ÇäÊú¨Ê†ºÁöÑ„Å™„Éì„ÉÉ„Ç∞„Éá„Éº„Çø„Å´„ÇÇÂØæÂøúÂá∫Êù•„Çã
  - (‰ªäÂõûÁ¥π‰ªã„Åô„Çã„Çà„ÅÜ„Å™„ÇÑ„ÇäÊñπ„Åß„ÄÅ„É©„Ç§„Éà„Å´‰Ωø„ÅÜ„Åì„Å®„ÇÇÂèØËÉΩ)
- Java „Çí„Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ„Å´Âãï„Åè„Åì„Å®„Å´Âä†„Åà„ÄÅÊú¨Ê†ºÁöÑ„Å´Âãï„Åã„ÅôÂ†¥Âêà„ÅØ„ÇØ„É©„Çπ„Çø„ÉºÊßãÁØâ„Å™„Å©„ÅåÂøÖË¶Å„Å´„Å™„Çã„Åü„ÇÅ„ÄÅ„Åù„ÅÆ„É¨„Éô„É´„Åß„ÇÑ„Çã„Å®Âãï‰ΩúÁí∞Â¢É„ÅÆÊßãÁØâ„ÅÆ„Éè„Éº„Éâ„É´„ÅØÈ´ò„ÅÑ
- pandas„ÄÅnumpy „Å®„ÅÆË¶™ÂíåÊÄß„ÅØ dask „ÅÆÊñπ„Åå~~È´ò„ÅÑ~~È´ò„Åã„Å£„Åü
  - dask „ÅØÈÅÖÂª∂Ë©ï‰æ°„Åï„Çå„Çã‰ª•Â§ñ„ÅØÂêå„Åò„É°„ÇΩ„ÉÉ„Éâ„Çí„Åù„ÅÆ„Åæ„Åæ‰Ωø„Åà„Çã„Ç±„Éº„Çπ„ÅåÂ§ö„ÅÑ
  - spark „ÅØ„ÇÄ„Åó„Çç SQL „Å®‰∫íÊèõÊÄß„Åå„ÅÇ„Çã
  - „Åü„Å†„ÄÅÂÖàËø∞„ÅÆÈÄö„Çä pandas API „ÅåÊï¥ÂÇô„Åï„Çå„Å§„Å§„ÅÇ„Çã„ÅÆ„Åß„ÄÅpyspark „ÇÇ„ÅÇ„ÇãÁ®ãÂ∫¶ pandas „ÅÆ‰Ωø„ÅÑÊñπ„Åù„ÅÆ„Åæ„Åæ„Åß‰Ωø„Åà„Çã„Çà„ÅÜ„Å´„Å™„Å£„Å¶„ÅÑ„ÇãÊ®°Êßò

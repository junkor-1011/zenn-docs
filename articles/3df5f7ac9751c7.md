---
title: "pysparkå…¥é–€"
emoji: "ğŸ‡"
type: "tech"
topics:
  - "python"
  - "pyspark"
  - "spark"
  - "pandas"
published: true
published_at: "2023-04-04 01:37"
---

â€»Jupyter Notebook å½¢å¼ã§å®Ÿè¡Œï¼ˆ[åŸæœ¬](https://gist.github.com/junkor-1011/d54da7ec98a56c0905952daba82a2b4b#file-pyspark-test-ipynb)ï¼‰ã—ã€Markdown å½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸã‚‚ã®ã‚’ä¸€éƒ¨æ”¹å¤‰ã—ã¦ä½œã£ã¦ã„ã‚‹ã®ã§è‹¥å¹²è¦‹ã¥ã‚‰ã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã”å®¹èµ¦ãã ã•ã„

## æ¦‚è¦

pandas ãªã©ã¯å°è¦æ¨¡~ä¸­è¦æ¨¡ãã‚‰ã„ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‡¦ç†ã‚„åˆ†æã«ã¯æœ‰åŠ¹ã ãŒã€
ãƒ¡ãƒ¢ãƒªã«ä¹—ã‚Šåˆ‡ã‚‰ãªã„ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿é‡ã¯å‡¦ç†ã§ããªã„ã€‚
ã¾ãŸã€GIL ã«ã‚ˆã£ã¦åŸºæœ¬çš„ã« 1 ã‚¹ãƒ¬ãƒƒãƒ‰ã§ã®å‹•ä½œã«ãªã‚‹ãŸã‚ã€ãƒã‚·ãƒ³ã‚¹ãƒšãƒƒã‚¯ã‚’æœ€å¤§é™æ´»ã‹ã—åˆ‡ã‚‹ã®ã¯é›£ã—ã„ã€‚

é…å»¶è©•ä¾¡ã€ä¸¦åˆ—åˆ†æ•£å‡¦ç†ã«ã‚ˆã£ã¦æ‰€è¬‚ãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿ã¨ã„ã‚ã‚Œã‚‹ãƒ¬ãƒ™ãƒ«ã®ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ãƒ»åˆ†æã«ä½¿ã†ã“ã¨ãŒã§ãã€
æ›´ã« pandas ã¨ã®é€£æºãƒ»ä½µç”¨ãŒã§ãã‚‹ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦[pyspark](https://www.databricks.com/jp/glossary/pyspark)ãŒå­˜åœ¨ã™ã‚‹ãŸã‚ç´¹ä»‹ã™ã‚‹ã€‚

ãªãŠã€è©³ç´°ã¯[PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)ãªã©ã‚‚å‚ç…§ã®ã“ã¨

## ç’°å¢ƒã®ä½œæˆ

Mambaforge ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã® Linux ç’°å¢ƒã‚’å‰æã¨ã™ã‚‹ã€‚

- conda 23.1.0
- mamba 1.4.1

ä»¥ä¸‹ã®ã‚ˆã†ãª conda ä»®æƒ³ç’°å¢ƒä½œæˆç”¨ã® yaml ã‚’ç”¨æ„ã™ã‚‹:

```yaml:pyspark_env.yaml
# pyspark_env.yaml
name: pyspark-intro
channels:
  - conda-forge
dependencies:
  - python=3.11
  - pyspark=3.3
  - openjdk=17
  - pandas=1.5
  - pyarrow
  # for testdata
  - numpy
  - seaborn
  # jupyter
  - jupyterlab
```

[ï¼ˆå‚è€ƒï¼‰](https://qiita.com/junkor-1011/items/7ec9bfaaf76568ce4a05)

â€»å…ˆæ—¥[pandas ã® 2.0 ãŒãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸ](https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html)ãŒã€
deprecated ã«ãªã£ã¦ã„ãŸ[`iteritems`](https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html#removal-of-prior-version-deprecations-changes)ãŒå‰Šé™¤ã•ã‚ŒãŸå½±éŸ¿ã§ã€
pandas ã‹ã‚‰ pyspark ã® DataFrame ã«å¤‰æ›ã™ã‚‹éƒ¨åˆ†ãŒä¸€éƒ¨å‹•ä½œã—ãªããªã‚‹ç¾è±¡ãŒèµ·ã“ã‚‹ãŸã‚ã€å½“é¢ã¯ 1.5 ç³»ã‚’ä½¿ã†ã€‚

mamba ã‚³ãƒãƒ³ãƒ‰ã§ä»®æƒ³ç’°å¢ƒã‚’ä½œæˆ(conda ã ã¨ä¾å­˜é–¢ä¿‚ã®è§£æ±ºãŒé…ã„ã®ã§ mamba ã®ä½¿ç”¨ã‚’æ¨å¥¨)

```sh
mamba env create -f pyspark_env.yaml

# å‡ºæ¥ä¸ŠãŒã£ãŸç’°å¢ƒ(pyspark-intro)ã‚’activate
conda activate pyspark-intro
# or `mamba activate pyspark-intro`
```

## å®Ÿè¡Œ

- spark session ã®èµ·å‹•
- ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ä¿å­˜
- pandas ã¨ã®é€£æº

ã‚ãŸã‚Šã‚’ç°¡å˜ã«ç¢ºèªã™ã‚‹ã€‚

ãƒ‡ãƒ¼ã‚¿å‡¦ç†å‘¨ã‚Šã®è©³ç´°ã¯çœç•¥ã™ã‚‹ãŒã€Spark ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æ©Ÿèƒ½ãŒ[Spark SQL](https://www.databricks.com/jp/glossary/what-is-spark-sql)ã¨å¯¾å¿œã—ã¦ã„ã‚‹ã“ã¨ã‹ã‚‰ã€
æ¦‚ã­ SQL ã§å‡ºæ¥ã‚‹ã“ã¨ã‚ˆã†ãªã“ã¨ã‚’å®Ÿè¡Œå‡ºæ¥ã‚‹ã¨è€ƒãˆã‚Œã° OKã€‚ï¼ˆã¨ã„ã†ã‹ SQL ã§ã‚‚æ›¸ã‘ã‚‹ï¼‰

ä»¥ä¸‹ã€ä¸Šè¨˜ã®æ‰‹é †ã§ä½œã£ãŸ conda ä»®æƒ³ç’°å¢ƒã‹ã‚‰ Jupyter ã‚’èµ·å‹•ã—ã¦å®Ÿè¡Œã™ã‚‹ã€‚

```python
from pyspark.sql.session import SparkSession

# spark sessionã®ç«‹ã¡ä¸Šã’
spark = (SparkSession.builder
  .master("local[*]")
  .appName("LocalTest")
  .getOrCreate()
)
spark
```

![](https://storage.googleapis.com/zenn-user-upload/58066df2aa11-20230404.png)

â†“pandas ã¨ã®é€£æºã®ãŸã‚ã€config ã‚’èª¿æ•´ã—ã¦ãŠã(pyarrow ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãŠã)

å‚è€ƒ: https://learn.microsoft.com/ja-jp/azure/databricks/pandas/pyspark-pandas-conversion

```python
spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "true")
```

Spark Session ã‚’èµ·å‹•ã™ã‚‹ã¨ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯[http://localhost:4040](http://localhost:4040)ã«[Spark UI](http://localhost:4040)ãŒç«‹ã¡ä¸ŠãŒã‚Šã€
å®Ÿè¡ŒçŠ¶æ…‹ãªã©ã‚’ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ãŒå‡ºæ¥ã‚‹ï¼ˆãƒ‡ãƒãƒƒã‚°ãªã©ã«ã‚‚ä¾¿åˆ©ï¼‰

```python
# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
import seaborn as sns

iris = sns.load_dataset("iris")
display(iris)

iris.to_csv("testdata/iris.csv", index=False)
```

![](https://storage.googleapis.com/zenn-user-upload/99c390c6f8e3-20230404.png)

```python
# pysparkã§csvã‚’èª­ã¿è¾¼ã¿

df_iris = spark.read.csv(
    "testdata/iris.csv",
    header=True,
    inferSchema=True,
)

# é…å»¶è©•ä¾¡ã™ã‚‹ã®ã§ã€ã‚«ãƒ©ãƒ ã®ã‚¹ã‚­ãƒ¼ãƒã®ã¿è¡¨ç¤º
display(df_iris)
```

    DataFrame[sepal_length: string, sepal_width: string, petal_length: string, petal_width: string, species: string]

â€»(è£œè¶³):

- é…å»¶è©•ä¾¡ã‚’è¡Œã†é–¢ä¿‚ã§ã€ã‚ªãƒ³ãƒ¡ãƒ¢ãƒªã«å±•é–‹ã™ã‚‹ pandas ãªã©ã¨ç•°ãªã‚Š Spark ã®å‡¦ç†ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®å½¢å¼ã®å½±éŸ¿ã‚’ã‚‚ã‚ã«å—ã‘ã‚‹
  - ãƒ‡ãƒ¼ã‚¿å½¢å¼(csv/json/parquet/orc/RDB ãƒ†ãƒ¼ãƒ–ãƒ«/...etc)ã‚„ã€åœ§ç¸®å½¢å¼(gzip/xz/snappy/zlib/...etc)ã€ã‚«ãƒ©ãƒ ã«å¯¾ã™ã‚‹ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãªã©
- ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»ãƒ‡ãƒ¼ã‚¿å‡¦ç†ç”¨é€”ã§ã‚ã‚Œã°è¡ŒæŒ‡å‘å½¢å¼ã§ã‚ã‚‹ csv ã‚’èª­ã¿è¾¼ã‚“ã§å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ã®ã¯ä¸€èˆ¬ã«é…ãã€åˆ—æŒ‡å‘å½¢å¼ã® parquet ã‚„ orc ãªã©ã®æ–¹ãŒæœ‰åˆ©

```python
# showãªã©ã«ã‚ˆã£ã¦åˆã‚ã¦è©•ä¾¡ãŒè¡Œã‚ã‚Œã‚‹
df_iris.show(5) # 5ä»¶ã‚’è¡¨ç¤º
```

    +------------+-----------+------------+-----------+-------+
    |sepal_length|sepal_width|petal_length|petal_width|species|
    +------------+-----------+------------+-----------+-------+
    |         5.1|        3.5|         1.4|        0.2| setosa|
    |         4.9|        3.0|         1.4|        0.2| setosa|
    |         4.7|        3.2|         1.3|        0.2| setosa|
    |         4.6|        3.1|         1.5|        0.2| setosa|
    |         5.0|        3.6|         1.4|        0.2| setosa|
    +------------+-----------+------------+-----------+-------+
    only showing top 5 rows

```python
# selectã«ã‚ˆã‚‹ã‚«ãƒ©ãƒ ã®é¸æŠ
(df_iris
    .select(["sepal_length", "sepal_width"])
    .show(3)
)
```

    +------------+-----------+
    |sepal_length|sepal_width|
    +------------+-----------+
    |         5.1|        3.5|
    |         4.9|        3.0|
    |         4.7|        3.2|
    +------------+-----------+
    only showing top 3 rows

```python
# filterã«ã‚ˆã‚‹ç°¡å˜ãªã‚¯ã‚¨ãƒª
(df_iris
    .filter(df_iris.species == "virginica")
    .filter(df_iris.sepal_length > 5.0)
    .show(3)
)
```

    +------------+-----------+------------+-----------+---------+
    |sepal_length|sepal_width|petal_length|petal_width|  species|
    +------------+-----------+------------+-----------+---------+
    |         6.3|        3.3|         6.0|        2.5|virginica|
    |         5.8|        2.7|         5.1|        1.9|virginica|
    |         7.1|        3.0|         5.9|        2.1|virginica|
    +------------+-----------+------------+-----------+---------+
    only showing top 3 rows

```python
# groupByã«ã‚ˆã‚‹é›†ç´„(1)
(df_iris
    .groupBy('species')
    .count()
    .show()
)
```

    +----------+-----+
    |   species|count|
    +----------+-----+
    | virginica|   50|
    |versicolor|   50|
    |    setosa|   50|
    +----------+-----+

```python
# groupByã«ã‚ˆã‚‹é›†ç´„(2)

(df_iris
    .groupBy('species')
    .agg({
        'sepal_length': 'mean',
        'sepal_width': 'mean',
        'petal_length': 'mean',
        'petal_width': 'mean',
    })
    .show()
)
```

    +----------+------------------+------------------+-----------------+------------------+
    |   species|  avg(sepal_width)|  avg(petal_width)|avg(sepal_length)| avg(petal_length)|
    +----------+------------------+------------------+-----------------+------------------+
    | virginica|2.9739999999999998|             2.026|6.587999999999998|             5.552|
    |versicolor|2.7700000000000005|1.3259999999999998|            5.936|              4.26|
    |    setosa| 3.428000000000001|0.2459999999999999|5.005999999999999|1.4620000000000002|
    +----------+------------------+------------------+-----------------+------------------+

```python
# DataFrameã®ã‚µãƒãƒªãƒ¼
df_iris.describe().show()
```

    +-------+------------------+-------------------+------------------+------------------+---------+
    |summary|      sepal_length|        sepal_width|      petal_length|       petal_width|  species|
    +-------+------------------+-------------------+------------------+------------------+---------+
    |  count|               150|                150|               150|               150|      150|
    |   mean| 5.843333333333335|  3.057333333333334|3.7580000000000027| 1.199333333333334|     null|
    | stddev|0.8280661279778637|0.43586628493669793|1.7652982332594662|0.7622376689603467|     null|
    |    min|               4.3|                2.0|               1.0|               0.1|   setosa|
    |    max|               7.9|                4.4|               6.9|               2.5|virginica|
    +-------+------------------+-------------------+------------------+------------------+---------+

```python
# SQLã§ã‚¯ã‚¨ãƒªã‚’æ›¸ãã“ã¨ã‚‚å¯èƒ½
df_iris.createOrReplaceTempView('iris')

spark.sql("show tables").show()

tmp = spark.sql("SELECT * FROM iris WHERE species = 'setosa'")
tmp.show()
```

    +---------+---------+-----------+
    |namespace|tableName|isTemporary|
    +---------+---------+-----------+
    |         |     iris|       true|
    +---------+---------+-----------+

    +------------+-----------+------------+-----------+-------+
    |sepal_length|sepal_width|petal_length|petal_width|species|
    +------------+-----------+------------+-----------+-------+
    |         5.1|        3.5|         1.4|        0.2| setosa|
    |         4.9|        3.0|         1.4|        0.2| setosa|
    |         4.7|        3.2|         1.3|        0.2| setosa|
    |         4.6|        3.1|         1.5|        0.2| setosa|
    |         5.0|        3.6|         1.4|        0.2| setosa|
    |         5.4|        3.9|         1.7|        0.4| setosa|
    |         4.6|        3.4|         1.4|        0.3| setosa|
    |         5.0|        3.4|         1.5|        0.2| setosa|
    |         4.4|        2.9|         1.4|        0.2| setosa|
    |         4.9|        3.1|         1.5|        0.1| setosa|
    |         5.4|        3.7|         1.5|        0.2| setosa|
    |         4.8|        3.4|         1.6|        0.2| setosa|
    |         4.8|        3.0|         1.4|        0.1| setosa|
    |         4.3|        3.0|         1.1|        0.1| setosa|
    |         5.8|        4.0|         1.2|        0.2| setosa|
    |         5.7|        4.4|         1.5|        0.4| setosa|
    |         5.4|        3.9|         1.3|        0.4| setosa|
    |         5.1|        3.5|         1.4|        0.3| setosa|
    |         5.7|        3.8|         1.7|        0.3| setosa|
    |         5.1|        3.8|         1.5|        0.3| setosa|
    +------------+-----------+------------+-----------+-------+
    only showing top 20 rows

è©³ç´°ã¯[User Guide](https://spark.apache.org/docs/3.3.2/api/python/user_guide/index.html), [API Reference](https://spark.apache.org/docs/3.3.2/api/python/reference/index.html),
ãŠã‚ˆã³[Spark SQL ã®ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹](https://spark.apache.org/docs/3.3.2/api/sql/index.html)ã‚’å‚ç…§

æ¬¡ã«ã€ãƒ‡ãƒ¼ã‚¿ã®[Read/Write](https://spark.apache.org/docs/latest/sql-data-sources.html)ã‚’ç¢ºèªã™ã‚‹

```python
# ã•ã£ãã¯ä¾¿å®œä¸Šcsvã‹ã‚‰èª­ã¿è¾¼ã‚“ã ãŒã€å‹æƒ…å ±ãŒå¤±ã‚ã‚Œã‚‹ + pandasã¨ã®é€£æºã‚’è¦‹ã‚‹ãŸã‚ã€pandasã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ç›´æ¥pysparkã«å¤‰æ›ã™ã‚‹

display(iris.__class__) # irisã¯pandas dataframe
df = spark.createDataFrame(iris)

display(df)

df.show()
```

    /home/wsl-user/LocalApps/Mambaforge/envs/pyspark-intro/lib/python3.11/site-packages/pyspark/pandas/__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.
      warnings.warn(



    pandas.core.frame.DataFrame


    /home/wsl-user/LocalApps/Mambaforge/envs/pyspark-intro/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.
      [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]



    DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string]


    +------------+-----------+------------+-----------+-------+
    |sepal_length|sepal_width|petal_length|petal_width|species|
    +------------+-----------+------------+-----------+-------+
    |         5.1|        3.5|         1.4|        0.2| setosa|
    |         4.9|        3.0|         1.4|        0.2| setosa|
    |         4.7|        3.2|         1.3|        0.2| setosa|
    |         4.6|        3.1|         1.5|        0.2| setosa|
    |         5.0|        3.6|         1.4|        0.2| setosa|
    |         5.4|        3.9|         1.7|        0.4| setosa|
    |         4.6|        3.4|         1.4|        0.3| setosa|
    |         5.0|        3.4|         1.5|        0.2| setosa|
    |         4.4|        2.9|         1.4|        0.2| setosa|
    |         4.9|        3.1|         1.5|        0.1| setosa|
    |         5.4|        3.7|         1.5|        0.2| setosa|
    |         4.8|        3.4|         1.6|        0.2| setosa|
    |         4.8|        3.0|         1.4|        0.1| setosa|
    |         4.3|        3.0|         1.1|        0.1| setosa|
    |         5.8|        4.0|         1.2|        0.2| setosa|
    |         5.7|        4.4|         1.5|        0.4| setosa|
    |         5.4|        3.9|         1.3|        0.4| setosa|
    |         5.1|        3.5|         1.4|        0.3| setosa|
    |         5.7|        3.8|         1.7|        0.3| setosa|
    |         5.1|        3.8|         1.5|        0.3| setosa|
    +------------+-----------+------------+-----------+-------+
    only showing top 20 rows

â†‘csv ã‹ã‚‰èª­ã¿è¾¼ã‚“ã ã¨ãã¯ string å‹ã«ãªã£ã¦ã—ã¾ã£ã¦ã„ãŸæ•°å€¤ãŒã€pandas ã®å‹ã‚’åæ˜ ã—ã¦ double å‹ã«ãªã£ã¦ã„ã‚‹

```python
# spark dataframeã‹ã‚‰pandas dataframeã¸ã®å¤‰æ›

display(df.__class__) # dfã¯pyspark dataframe

pdf = df.toPandas()
display(pdf)
```

    pyspark.sql.dataframe.DataFrame

![](https://storage.googleapis.com/zenn-user-upload/e3a1d8216c07-20230404.png)

â†‘pyspark ã§å¤§è¦æ¨¡~ä¸­è¦æ¨¡ãã‚‰ã„ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¦ã‚µã‚¤ã‚ºãƒ€ã‚¦ãƒ³ã—ã€ã‚ªãƒ³ãƒ¡ãƒ¢ãƒªã«ä¹—ã‚‹ãã‚‰ã„ã«ãªã£ãŸã‚‰`toPandas`ã‚’ã—ã¦æ‰±ã„ã‚„ã™ã„ pandas ã«å¤‰æ›ã™ã‚‹ã€ã¨ã„ã†ä½¿ã„æ–¹ãŒå‡ºæ¥ã‚‹ã€‚

æœ€å¾Œã«è‰²ã€…ãªå½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ã®æ›¸ãè¾¼ã¿ã¨èª­ã¿è¾¼ã¿ã‚’è¡Œã†

å…ˆè¿°ã®é€šã‚Šã€spark ã¯ãƒ‡ãƒ¼ã‚¿ä¿å­˜å½¢å¼ãŒå‡¦ç†ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«ãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã«åŠ¹ã„ã¦ãã‚‹ãŸã‚ã€ã“ã®è¾ºã‚Šã®å–ã‚Šæ‰±ã„ã¯è‚ã«ãªã£ã¦ãã‚‹ã€‚

```python
# dfã¯spark dataframe

# csv.gz
df.write.save("testdata/iris_csv", format="csv")

# formatã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ã†ä»£ã‚ã‚Šã«ã€â†“ã®ã‚ˆã†ã«æ›¸ã„ã¦ã‚‚OK
# df.write.csv("testdata/iris_csv", compression="gzip")

# å®Ÿéš›ã©ã®ã‚ˆã†ãªå½¢å¼ã§ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª(linuxã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ã„ã‚‹)
!ls testdata/iris_csv
```

    _SUCCESS
    part-00000-030e186f-051c-4e9d-94b1-1560776ebfef-c000.csv
    part-00001-030e186f-051c-4e9d-94b1-1560776ebfef-c000.csv
    part-00002-030e186f-051c-4e9d-94b1-1560776ebfef-c000.csv
    part-00003-030e186f-051c-4e9d-94b1-1560776ebfef-c000.csv
    part-00004-030e186f-051c-4e9d-94b1-1560776ebfef-c000.csv
    part-00005-030e186f-051c-4e9d-94b1-1560776ebfef-c000.csv
    part-00006-030e186f-051c-4e9d-94b1-1560776ebfef-c000.csv
    part-00007-030e186f-051c-4e9d-94b1-1560776ebfef-c000.csv

```python
# ä¸€å¿œä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®1ã¤ã«ã¤ã„ã¦å…ˆé ­ã‚’è¦‹ã¦ã¿ã‚‹
!head testdata/iris_csv/part-00000-*.csv
```

    5.1,3.5,1.4,0.2,setosa
    4.9,3.0,1.4,0.2,setosa
    4.7,3.2,1.3,0.2,setosa
    4.6,3.1,1.5,0.2,setosa
    5.0,3.6,1.4,0.2,setosa
    5.4,3.9,1.7,0.4,setosa
    4.6,3.4,1.4,0.3,setosa
    5.0,3.4,1.5,0.2,setosa
    4.4,2.9,1.4,0.2,setosa
    4.9,3.1,1.5,0.1,setosa

â†‘pandas ã®ã‚ˆã†ã«å˜ä¸€ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã™ã‚‹æ„Ÿã˜ã§ã¯ãªãã€ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã™ã‚‹ã€‚
(ãƒ•ã‚¡ã‚¤ãƒ«ãŒåˆ†æ•£ã—ã¦ç”Ÿæˆã•ã‚Œã‚‹)

èª­ã¿è¾¼ã‚€éš›ã¯å€‹ã€…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚‚ã§ãã‚‹(â†Spark ä»¥å¤–ã§ä½œã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ãªã©)ãŒã€
**åŸºæœ¬ã¯ä¿å­˜ã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã—ã¦èª­ã¿è¾¼ã‚€**

```python
df_csv = spark.read.load("testdata/iris_csv", format="csv")

# formatã‚’æŒ‡å®šã›ãšã€â†“ã®ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½
# df_csv = spark.read.csv("testdata/iris_csv")

df_csv.show(3)
```

    +---+---+---+---+----------+
    |_c0|_c1|_c2|_c3|       _c4|
    +---+---+---+---+----------+
    |4.9|2.4|3.3|1.0|versicolor|
    |6.6|2.9|4.6|1.3|versicolor|
    |5.2|2.7|3.9|1.4|versicolor|
    +---+---+---+---+----------+
    only showing top 3 rows

(â†‘csv ã ã¨ã‚«ãƒ©ãƒ åãªã©ã®æƒ…å ±ãŒæ¬ è½ã—ãŒã¡ã§ä½¿ã„ã¥ã‚‰ã„)

```python
# ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã ã¨csvã®ä»–ã«jsonãªã©ã‚‚æŒ‡å®šå¯èƒ½
# compressionã§åœ§ç¸®å½¢å¼ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã‚‚å‡ºæ¥ã‚‹

df.write.json("testdata/iris_json", compression='gzip')

!ls testdata/iris_json
```

    _SUCCESS
    part-00000-005412a7-1f0e-4ebf-a4d9-bfbbe53b0515-c000.json.gz
    part-00001-005412a7-1f0e-4ebf-a4d9-bfbbe53b0515-c000.json.gz
    part-00002-005412a7-1f0e-4ebf-a4d9-bfbbe53b0515-c000.json.gz
    part-00003-005412a7-1f0e-4ebf-a4d9-bfbbe53b0515-c000.json.gz
    part-00004-005412a7-1f0e-4ebf-a4d9-bfbbe53b0515-c000.json.gz
    part-00005-005412a7-1f0e-4ebf-a4d9-bfbbe53b0515-c000.json.gz
    part-00006-005412a7-1f0e-4ebf-a4d9-bfbbe53b0515-c000.json.gz
    part-00007-005412a7-1f0e-4ebf-a4d9-bfbbe53b0515-c000.json.gz

```python
# èª­ã¿è¾¼ã¿ã‚‚åŒæ§˜

df_json = spark.read.json("testdata/iris_json")
# or df_json = spark.read.load("testdata/iris_json", format="json")

display(df_json)
df_json.show(3)
```

    DataFrame[petal_length: double, petal_width: double, sepal_length: double, sepal_width: double, species: string]


    +------------+-----------+------------+-----------+----------+
    |petal_length|petal_width|sepal_length|sepal_width|   species|
    +------------+-----------+------------+-----------+----------+
    |         4.2|        1.2|         5.7|        3.0|versicolor|
    |         4.2|        1.3|         5.7|        2.9|versicolor|
    |         4.3|        1.3|         6.2|        2.9|versicolor|
    +------------+-----------+------------+-----------+----------+
    only showing top 3 rows

(â†‘json ã ã¨ csv ã‚ˆã‚Šã¯å‹æƒ…å ±ãŒä¿æŒã•ã‚Œã‚‹)

```python
# ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ»å‡¦ç†ç”¨é€”ã¨ã—ã¦é©ã—ã¦ã„ã‚‹snappy.parquetã‚„zlib.orcå½¢å¼ãªã©

df.write.parquet("testdata/iris_parquet", compression="snappy")
!ls testdata/iris_parquet

df.write.orc("testdata/iris_orc", compression="zlib")
!ls testdata/iris_orc
```

    23/04/04 01:07:23 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory
    Scaling row group sizes to 95.00% for 8 writers




    _SUCCESS
    part-00000-ccc33d97-5a54-492a-9cd2-12ae8adb52ea-c000.snappy.parquet
    part-00001-ccc33d97-5a54-492a-9cd2-12ae8adb52ea-c000.snappy.parquet
    part-00002-ccc33d97-5a54-492a-9cd2-12ae8adb52ea-c000.snappy.parquet
    part-00003-ccc33d97-5a54-492a-9cd2-12ae8adb52ea-c000.snappy.parquet
    part-00004-ccc33d97-5a54-492a-9cd2-12ae8adb52ea-c000.snappy.parquet
    part-00005-ccc33d97-5a54-492a-9cd2-12ae8adb52ea-c000.snappy.parquet
    part-00006-ccc33d97-5a54-492a-9cd2-12ae8adb52ea-c000.snappy.parquet
    part-00007-ccc33d97-5a54-492a-9cd2-12ae8adb52ea-c000.snappy.parquet
    _SUCCESS
    part-00000-0fbad17c-8b1f-42ce-a3b2-06d173e191ed-c000.zlib.orc
    part-00001-0fbad17c-8b1f-42ce-a3b2-06d173e191ed-c000.zlib.orc
    part-00002-0fbad17c-8b1f-42ce-a3b2-06d173e191ed-c000.zlib.orc
    part-00003-0fbad17c-8b1f-42ce-a3b2-06d173e191ed-c000.zlib.orc
    part-00004-0fbad17c-8b1f-42ce-a3b2-06d173e191ed-c000.zlib.orc
    part-00005-0fbad17c-8b1f-42ce-a3b2-06d173e191ed-c000.zlib.orc
    part-00006-0fbad17c-8b1f-42ce-a3b2-06d173e191ed-c000.zlib.orc
    part-00007-0fbad17c-8b1f-42ce-a3b2-06d173e191ed-c000.zlib.orc

```python
# èª­ã¿è¾¼ã¿

# parquet
print("parquet:")
df_parquet = spark.read.parquet("testdata/iris_parquet")
display(df_parquet)
df_parquet.show(3)

print("\n------------------\n")

# orc
print("orc:")
df_orc = spark.read.orc("testdata/iris_orc")
display(df_orc)
df_orc.show(3)
```

    parquet:



    DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string]


    +------------+-----------+------------+-----------+----------+
    |sepal_length|sepal_width|petal_length|petal_width|   species|
    +------------+-----------+------------+-----------+----------+
    |         5.7|        3.0|         4.2|        1.2|versicolor|
    |         5.7|        2.9|         4.2|        1.3|versicolor|
    |         6.2|        2.9|         4.3|        1.3|versicolor|
    +------------+-----------+------------+-----------+----------+
    only showing top 3 rows


    ------------------

    orc:



    DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string]


    +------------+-----------+------------+-----------+----------+
    |sepal_length|sepal_width|petal_length|petal_width|   species|
    +------------+-----------+------------+-----------+----------+
    |         5.7|        3.0|         4.2|        1.2|versicolor|
    |         5.7|        2.9|         4.2|        1.3|versicolor|
    |         6.2|        2.9|         4.3|        1.3|versicolor|
    +------------+-----------+------------+-----------+----------+
    only showing top 3 rows

```python
# partitionã‚’åˆ‡ã£ã¦ä¿å­˜ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½

df.write.save(
    "testdata/iris_with_partition",
    format="parquet",
    compression="snappy",
    partitionBy="species"
    )

!cd testdata/iris_with_partition && tree
```

.
â”œâ”€â”€ \_SUCCESS
â”œâ”€â”€ species=setosa
â”‚ â”œâ”€â”€ part-00000-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet
â”‚ â”œâ”€â”€ part-00001-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet
â”‚ â””â”€â”€ part-00002-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet
â”œâ”€â”€ species=versicolor
â”‚ â”œâ”€â”€ part-00002-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet
â”‚ â”œâ”€â”€ part-00003-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet
â”‚ â”œâ”€â”€ part-00004-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet
â”‚ â””â”€â”€ part-00005-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet
â””â”€â”€ species=virginica
â”œâ”€â”€ part-00005-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet
â”œâ”€â”€ part-00006-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet
â””â”€â”€ part-00007-a847cdcf-1aea-4b71-a3cc-7ec2adddc8b0.c000.snappy.parquet

    4 directories, 11 files

ã‚«ãƒ©ãƒ "species"ã®å€¤ã”ã¨ã«åˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒåˆ‡ã‚‰ã‚Œã¦ãƒ‡ãƒ¼ã‚¿ãŒä¿å­˜ã•ã‚Œã‚‹

RDB ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨åŒæ§˜ã€ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ã®ä»•æ–¹ã«å¿œã˜ã¦é©åˆ‡ã«è¨­å®šã™ã‚‹ã“ã¨ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è‰¯ãã™ã‚‹ã“ã¨ãŒå‡ºæ¥ã‚‹ï¼ˆã—ã€ä¸é©åˆ‡ã ã¨æ‚ªåŒ–ã™ã‚‹ï¼‰

â†‘ ã®ä¾‹ã ã¨ã€species ã‚’æŒ‡å®šã—ã¦åˆ†æã‚’è¡Œã†ã‚ˆã†ãªå ´åˆã€èˆˆå‘³ã®ã‚ã‚‹ species ã—ã‹è¦‹ãªã„ãŸã‚ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãƒ‡ãƒ¼ã‚¿é‡ã‚’é™å®šã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚
ä¸€æ–¹ã€ã‹ã‘ã‚‹ã‚¯ã‚¨ãƒªã®å¤šããŒè¤‡æ•°ã® species ã«ã¾ãŸãŒã‚‹ã‚ˆã†ãªå ´åˆã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å‘ä¸Šã¯è¦‹è¾¼ã‚ãšã€æ‚ªåŒ–ã™ã‚‹å¯èƒ½æ€§ã‚‚ã‚ã‚‹ã€‚

```python
# ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹å ´åˆã§ã‚‚èª­ã¿è¾¼ã¿æ–¹æ³•ã¯åŒæ§˜

df_partition = spark.read.load("testdata/iris_with_partition")

display(df_partition)
df_partition.show(3)
```

    DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string]


    +------------+-----------+------------+-----------+---------+
    |sepal_length|sepal_width|petal_length|petal_width|  species|
    +------------+-----------+------------+-----------+---------+
    |         5.8|        2.8|         5.1|        2.4|virginica|
    |         6.4|        3.2|         5.3|        2.3|virginica|
    |         6.5|        3.0|         5.5|        1.8|virginica|
    +------------+-----------+------------+-----------+---------+
    only showing top 3 rows

ä»–ã«ã‚‚[saveAsTable](https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.DataFrameWriter.saveAsTable.html)ã§æ°¸ç¶šåŒ–ã—ãŸãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦æ›¸ãè¾¼ã¿ã‚’è¡Œã£ãŸã‚Šã‚‚å‡ºæ¥ã‚‹ã€‚

æ‰±ãˆã‚‹ãƒ‡ãƒ¼ã‚¿ä¿å­˜å½¢å¼ã‚‚[Data Sources](https://spark.apache.org/docs/latest/sql-data-sources.html)ã®ã‚ˆã†ã«ã€
ä»–ã«ã‚‚ RDB(JDBC ã‚’ä½¿ã†)ã‚„ã€Hive Table ãªã©è‰²ã€…ãªã‚‚ã®ã«å¯¾å¿œã—ã¦ã„ã‚‹ã€‚

## (è£œè¶³)pyspark ã«ãŠã‘ã‚‹ pandas API

å‚è€ƒ: https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/index.html

æœ€è¿‘ã¯ã‹ãªã‚Š pandas ã«è¿‘ã„ API ã‚’ä½¿ã£ã¦ spark ã‚’æ‰±ã†æ–¹æ³•ãŒæ•´å‚™ã•ã‚Œã¤ã¤ã‚ã‚‹æ§˜å­

ãªãŠã€[Koalas](https://github.com/databricks/koalas)ã¨ã„ã†ã€spark ã‚’ pandas ã£ã½ãä½¿ãˆã‚‹ãƒ„ãƒ¼ãƒ«ãŒå­˜åœ¨ã™ã‚‹ãŒã€
[ã“ã“](https://learn.microsoft.com/ja-jp/azure/databricks/archive/legacy/koalas)ãªã©ã‚’è¦‹ã‚‹ã¨ã“ã‚ŒãŒæ­£å¼ã« pyspark ã«å–ã‚Šè¾¼ã¾ã‚ŒãŸï¼Ÿæ§˜å­ã€‚

```python
import pyspark.pandas as ps

# pandas on spark DataFrameã‚’ä½œæˆã™ã‚‹
psdf = ps.read_csv("testdata/iris.csv")

display(psdf.__class__) # pyspark.pandas.frame.DataFrame

psdf.head(5) # pandas DataFrameã®ã‚ˆã†ã«headã§å…ˆé ­ã‚’è¡¨ç¤ºã§ãã‚‹
```

    /home/wsl-user/LocalApps/Mambaforge/envs/pyspark-intro/lib/python3.11/site-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.
      warnings.warn(message, PandasAPIOnSparkAdviceWarning)



    pyspark.pandas.frame.DataFrame

![](https://storage.googleapis.com/zenn-user-upload/d915a20ea87b-20230404.png)

```python
# filterã‚„è¡¨ç¤ºã‚‚é€šå¸¸ã®pandasã®ã‚ˆã†ã«ã§ãã¦ã„ã‚‹
psdf[psdf['sepal_length'] > 7.5]
```

![](https://storage.googleapis.com/zenn-user-upload/ba8a910f46ba-20230404.png)

```python
# pysparkã®DataFrameã«å¤‰æ›

sdf = psdf.to_spark(index_col=["index"])

sdf.show(5)
```

    +-----+------------+-----------+------------+-----------+-------+
    |index|sepal_length|sepal_width|petal_length|petal_width|species|
    +-----+------------+-----------+------------+-----------+-------+
    |    0|         5.1|        3.5|         1.4|        0.2| setosa|
    |    1|         4.9|        3.0|         1.4|        0.2| setosa|
    |    2|         4.7|        3.2|         1.3|        0.2| setosa|
    |    3|         4.6|        3.1|         1.5|        0.2| setosa|
    |    4|         5.0|        3.6|         1.4|        0.2| setosa|
    +-----+------------+-----------+------------+-----------+-------+
    only showing top 5 rows

```python
# æ™®é€šã®pandasã«å¤‰æ›
pdf = psdf.to_pandas()

display(pdf.__class__) # pandas.core.frame.DataFrame

pdf.head()
```

    /home/wsl-user/LocalApps/Mambaforge/envs/pyspark-intro/lib/python3.11/site-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.
      warnings.warn(message, PandasAPIOnSparkAdviceWarning)



    pandas.core.frame.DataFrame

![](https://storage.googleapis.com/zenn-user-upload/db76402dc9ee-20230404.png)

## (å‚è€ƒ) [dask](https://www.dask.org/)ã¨ã®æ¯”è¼ƒ

pandas ãŠã‚ˆã³ numpy ã¨äº’æ›æ€§ãŠã‚ˆã³é€£æºæ€§ã®é«˜ã„ API ã‚’æŒã¡ã€é…å»¶è©•ä¾¡ãƒ»ä¸¦åˆ—åˆ†æ•£å‡¦ç†ã‚’è¡Œã†ã“ã¨ãŒå‡ºæ¥ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚

ä¾‹ãˆã°ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå¤§ãã„ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã¯ dask ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã§é…å»¶è©•ä¾¡ãƒ»ä¸¦åˆ—åˆ†æ•£å‡¦ç†ã‚’è¡Œã£ã¦ãŠãã€
ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå°ã•ããªã‚‹ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ pandas ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›ã™ã‚‹ã€ã¨ã„ã£ãŸä»Šå› pyspark ã§ã‚„ã‚ã†ã¨ã—ã¦ã„ã‚‹ã“ã¨ã¨ã»ã¼åŒã˜ã“ã¨ãŒå‡ºæ¥ã‚‹ã€‚

pyspark ã¯ dask ã«æ¯”ã¹ã‚‹ã¨ã€

- ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãªã©ã‚’çµ„ã‚ã‚‹åˆ†ã€ã‚ˆã‚Šæœ¬æ ¼çš„ãªãƒ“ãƒƒã‚°ãƒ‡ãƒ¼ã‚¿ã«ã‚‚å¯¾å¿œå‡ºæ¥ã‚‹
  - (ä»Šå›ç´¹ä»‹ã™ã‚‹ã‚ˆã†ãªã‚„ã‚Šæ–¹ã§ã€ãƒ©ã‚¤ãƒˆã«ä½¿ã†ã“ã¨ã‚‚å¯èƒ½)
- Java ã‚’ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«å‹•ãã“ã¨ã«åŠ ãˆã€æœ¬æ ¼çš„ã«å‹•ã‹ã™å ´åˆã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ§‹ç¯‰ãªã©ãŒå¿…è¦ã«ãªã‚‹ãŸã‚ã€ãã®ãƒ¬ãƒ™ãƒ«ã§ã‚„ã‚‹ã¨å‹•ä½œç’°å¢ƒã®æ§‹ç¯‰ã®ãƒãƒ¼ãƒ‰ãƒ«ã¯é«˜ã„
- pandasã€numpy ã¨ã®è¦ªå’Œæ€§ã¯ dask ã®æ–¹ãŒ~~é«˜ã„~~é«˜ã‹ã£ãŸ
  - dask ã¯é…å»¶è©•ä¾¡ã•ã‚Œã‚‹ä»¥å¤–ã¯åŒã˜ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ãã®ã¾ã¾ä½¿ãˆã‚‹ã‚±ãƒ¼ã‚¹ãŒå¤šã„
  - spark ã¯ã‚€ã—ã‚ SQL ã¨äº’æ›æ€§ãŒã‚ã‚‹
  - ãŸã ã€å…ˆè¿°ã®é€šã‚Š pandas API ãŒæ•´å‚™ã•ã‚Œã¤ã¤ã‚ã‚‹ã®ã§ã€pyspark ã‚‚ã‚ã‚‹ç¨‹åº¦ pandas ã®ä½¿ã„æ–¹ãã®ã¾ã¾ã§ä½¿ãˆã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã‚‹æ¨¡æ§˜
